# -*- coding: utf-8 -*-
"""Restaruant Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EjG-aOYmJpTmc7bcWZqwckWSS2zuhKx4
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd

data = pd.read_csv('/content/drive/MyDrive/Restaurant_Reviews.tsv', delimiter='\t', quoting=1)

data.shape

data.columns

data.head()

data.info

import nltk # natural language toolkit is a package from NLP
import re # Regular expressions
nltk.download('stopwords') # NLTK
from nltk.corpus import stopwords # a stop word is commonly used word that search engine is programmed to ignore
from nltk.stem.porter import PorterStemmer # mainly focuses on Data Mining and information retrieval .... it is used to remove the suffixes from an English word and obtain its stem

corpus = []
for i in range(0,1000):
  review = re.sub(pattern='[^a-zA-Z]', repl=' ', string=data['Review'][i]) # removes and replaces every non 'a-z' and 'A-Z' with ' '.
  review = review.lower() # to lower case
  review_words = review.split() # tokenizing review by words
  review_words = [word for word in review_words if not word in set(stopwords.words('english'))] # every non-stopword is added to the list ( i.e stopwords such as 'the', 'this' etc are omitted).
  ps = PorterStemmer()
  review = [ps.stem(word) for word in review_words]
  review = ' '.join(review)
  corpus.append(review)

len(corpus)

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier

# Replace CountVectorizer with TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=1500)
X = tfidf_vectorizer.fit_transform(corpus).toarray()


# from sklearn.feature_extraction.text import CountVectorizer # cv is used to convert text to numerical data
# cv = CountVectorizer(max_features=1500) # initialize cv to 1500(data size from corpus)
# in below fit_transform fits the vectorizer to the corpus (learns the vocabulary and tokenizes the documents) and then transforms the documents into a document-term matrix (DTM) which is then converted to an array.
# the DTM formed where each row represents a document, and each column represents a word from the vocabulary
# X = cv.fit_transform(corpus).toarray() # corpus is converted into numerical data as an array format
Y = data.iloc[:, 1].values # extracting the likes(boolean) from data(origianl set tsv)

# here splitting data is done into two parts (training and testing) so that to train the model to training dataset and test its accuracy on unseen test data
from sklearn.model_selection import train_test_split # uses to split original data into training data & test data
X_train, X_test, Y_train, Y_test, = train_test_split(X,Y,test_size=0.20,random_state =0) # test_size for splitting purpose and random state simply sets seed to the random generator so that train-test splits are always deterministic if not it will be different each time

# 80% - training dataset
# 20% - testing dataset

X_train.shape , X_test.shape, Y_train.shape, Y_test.shape

# PREPROCESSING PART ENDS---
# MODEL TRAING----
# Multinomial Naive bayes - the algorithm is a probabilistic learning method that is mostly used in NLP. Its based on Bayes theorem and predicts tag of text such as piece of email or newpaper article

# fitting Naive bayes to the training set
# from sklearn.naive_bayes import MultinomialNB
# MNB is suitable for the classification purpose with discrete features( word counts for text classification)

# Replace Multinomial Naive Bayes with Random Forest
from sklearn.ensemble import RandomForestClassifier

classifier = RandomForestClassifier(n_estimators=100, random_state=0)
classifier.fit(X_train, Y_train)


# classifier = MultinomialNB()
# classifier.fit(X_train, Y_train) # fitting training data

# Predicting test set results
# Y_pred = classifier.predict(X_test)
# Predictions using Random Forest
Y_pred = classifier.predict(X_test)

Y_pred

# for accuracy , precision and recall
from sklearn.metrics import accuracy_score # it calculates the accuracy score for a set of predictedd values against true labels.

from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

score_acc = accuracy_score(Y_test, Y_pred)
score_pre = precision_score(Y_test, Y_pred)
score_rec = recall_score(Y_test, Y_pred)

print("---Scores---")
print("Accuracy Score : {}% ".format(round(score_acc*100,2)))
print("Precision Score : {}% ".format(round(score_pre*100,2)))
print("Recall Score : {}% ".format(round(score_rec*100,2)))

# Making the Confusion Matrix
# A Confusion Matrix is a table that is used to define the performance of a classification algorithm. A Confusion Matrix visualizes and summarizes the performance of a classification

from sklearn.metrics import confusion_matrix
# (test , predicted)
cm = confusion_matrix(Y_test, Y_pred)
# cm -> array -> [[true +ve, false +ve] , [false -ve, true -ve]

cm

# Commented out IPython magic to ensure Python compatibility.
from ast import increment_lineno
# plotting confusion matrix

import matplotlib.pyplot as plt # a low level graph plotting library for visualizing purpose
import seaborn as sns # a visualizing library based on matplotlib
# %matplotlib inline

plt.figure(figsize=(10,6))
sns.heatmap(cm, annot=True, cmap= 'YlGnBu' , xticklabels=['Negative','Positive'], yticklabels=['Negative','Positive'])
plt.xlabel("Predicted Values")
plt.ylabel("Actual Values")

# Hyperparameter tuning for Random Forest Classifier
best_accuracy_rf = 0.0
best_n_estimators = 0

for n_estimators in range(50, 201, 50):
    temp_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=0)
    temp_classifier.fit(X_train, Y_train)
    temp_Y_pred = temp_classifier.predict(X_test)
    score = accuracy_score(Y_test, temp_Y_pred)
    
    print("Accuracy Score for n_estimators={} is: {}%".format(n_estimators, round(score * 100, 2)))
    
    if score > best_accuracy_rf:
        best_accuracy_rf = score
        best_n_estimators = n_estimators

print('------------')
print('The Best Accuracy for Random Forest is {}% with n_estimators as {}'.format(round(best_accuracy_rf * 100, 2), best_n_estimators))

# Fit the final Random Forest classifier with the best hyperparameters
final_classifier = RandomForestClassifier(n_estimators=best_n_estimators, random_state=0)
final_classifier.fit(X_train, Y_train)


# PREDICTIONS

def predict_review(sample_review):
    sample_review = re.sub(pattern='[^a-zA-Z]', repl=' ', string=sample_review)
    sample_review = sample_review.lower()
    sample_review_words = sample_review.split()
    sample_review_words = [word for word in sample_review_words if not word in set(stopwords.words('english'))]
    ps = PorterStemmer()
    final_review = [ps.stem(word) for word in sample_review_words]
    final_review = ' '.join(final_review)

    # Use the same TF-IDF vectorizer to transform the input review
    temp = tfidf_vectorizer.transform([final_review]).toarray()
    return final_classifier.predict(temp)


sample_review = "My recent visit to the restaurant left me thoroughly disappointed. The confusing menu, filled with obscure descriptions, made ordering a daunting task. The lackluster food, served tepid and tasteless, only added to my dismay. The inattentive staff's incompetence further marred the experience, leaving me eager to leave."
if predict_review(sample_review):
  print('Positive review')
else:
  print('Negative review')

sample_review = "My recent dining experience at the restaurant was nothing short of extraordinary. From the moment I stepped inside, I was enveloped in an ambiance that exuded warmth and sophistication. The menu, a symphony of culinary creativity, featured a diverse array of dishes that promised an exciting gastronomic journey.Each plate that graced our table was a masterpiece of flavor and presentation. The chef's meticulous attention to detail was evident in every bite, as fresh, locally sourced ingredients melded harmoniously to create a symphony of tastes. Whether it was the perfectly seared steak, the delicate seafood, or the vibrant vegetarian options, each dish was a testament to the chef's culinary prowess.The service was impeccable, with an attentive and knowledgeable staff that enhanced our dining experience. They guided us through the menu, offering thoughtful recommendations and ensuring our glasses were never empty.In conclusion, my visit to this restaurant was an exquisite culinary adventure. It exceeded all expectations, leaving me with memories of a remarkable dining experience I will treasure. I wholeheartedly recommend this establishment to anyone seeking an unforgettable gastronomic journey."
if predict_review(sample_review):
  print('Positive review')
else:
  print('Negative review')

# # other method with more advanced neural network model 
# # Import necessary libraries
# import numpy as np
# import pandas as pd
# import nltk
# import re
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
# import matplotlib.pyplot as plt
# import seaborn as sns
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.preprocessing import LabelEncoder
# import tensorflow as tf
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Embedding, Dense, Flatten
# from tensorflow.keras.preprocessing.text import Tokenizer
# from tensorflow.keras.preprocessing.sequence import pad_sequences

# # Load the dataset
# data = pd.read_csv('/content/drive/MyDrive/Restaurant_Reviews.tsv', delimiter='\t', quoting=1)

# # Data preprocessing
# nltk.download('stopwords')
# from nltk.corpus import stopwords
# from nltk.stem.porter import PorterStemmer

# corpus = []
# for i in range(0, 1000):
#     review = re.sub(pattern='[^a-zA-Z]', repl=' ', string=data['Review'][i])
#     review = review.lower()
#     review_words = review.split()
#     review_words = [word for word in review_words if not word in set(stopwords.words('english'))]
#     ps = PorterStemmer()
#     review = ' '.join(review_words)
#     corpus.append(review)

# # Split data into features and labels
# X = corpus
# Y = data.iloc[:, 1].values

# # TF-IDF Vectorization
# tfidf_vectorizer = TfidfVectorizer(max_features=1500)
# X_tfidf = tfidf_vectorizer.fit_transform(X)

# # Encode labels (Positive: 1, Negative: 0)
# label_encoder = LabelEncoder()
# Y_encoded = label_encoder.fit_transform(Y)

# # Split the data into training and testing sets
# X_train, X_test, Y_train, Y_test = train_test_split(X_tfidf, Y_encoded, test_size=0.20, random_state=0)

# # Define and train a simple neural network
# model = Sequential()
# model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
# model.add(Dense(1, activation='sigmoid'))
# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# # Train the model
# history = model.fit(X_train.toarray(), Y_train, epochs=10, batch_size=32, validation_split=0.1, verbose=1)

# # Evaluate the model
# Y_pred = model.predict(X_test.toarray())
# Y_pred = (Y_pred > 0.5).astype(int)

# accuracy = accuracy_score(Y_test, Y_pred)
# precision = precision_score(Y_test, Y_pred)
# recall = recall_score(Y_test, Y_pred)

# print("---Scores---")
# print("Accuracy Score : {:.2f}%".format(accuracy * 100))
# print("Precision Score : {:.2f}%".format(precision * 100))
# print("Recall Score : {:.2f}%".format(recall * 100))

# # Confusion Matrix
# cm = confusion_matrix(Y_test, Y_pred)

# # Plot the confusion matrix
# plt.figure(figsize=(10, 6))
# sns.heatmap(cm, annot=True, cmap='YlGnBu', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
# plt.xlabel("Predicted Values")
# plt.ylabel("Actual Values")
# plt.show()



# # BERT MODEL which is more complex and require more gpu to run (16GB)
# # Install the Transformers library
# !pip install transformers

# import numpy as np
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
# import seaborn as sns
# import matplotlib.pyplot as plt
# import torch
# from torch.utils.data import DataLoader, TensorDataset
# from transformers import BertTokenizer, BertForSequenceClassification, AdamW

# # Load the dataset
# data = pd.read_csv('/content/drive/MyDrive/Restaurant_Reviews.tsv', delimiter='\t', quoting=1)

# # Split data into features and labels
# X = data['Review']
# Y = data['Liked']

# # Split the data into training and testing sets
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=0)

# # Load the pre-trained BERT tokenizer and model
# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# # Tokenize the text data
# def tokenize_text(text):
#     return tokenizer.encode_plus(
#         text,
#         add_special_tokens=True,
#         max_length=128,
#         pad_to_max_length=True,
#         return_attention_mask=True,
#         return_tensors='pt'
#     )

# X_train_tokens = [tokenize_text(text) for text in X_train]
# X_test_tokens = [tokenize_text(text) for text in X_test]

# # Create DataLoader for training and testing data
# batch_size = 32

# train_dataset = TensorDataset(
#     torch.cat([x['input_ids'] for x in X_train_tokens]),
#     torch.cat([x['attention_mask'] for x in X_train_tokens]),
#     torch.tensor(Y_train.values)
# )

# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# test_dataset = TensorDataset(
#     torch.cat([x['input_ids'] for x in X_test_tokens]),
#     torch.cat([x['attention_mask'] for x in X_test_tokens]),
#     torch.tensor(Y_test.values)
# )

# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# # Training parameters
# epochs = 5
# learning_rate = 2e-5

# # Use GPU if available
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# model.to(device)

# optimizer = AdamW(model.parameters(), lr=learning_rate)

# # Training loop
# for epoch in range(epochs):
#     model.train()
#     total_loss = 0
#     for batch in train_loader:
#         input_ids, attention_mask, labels = batch
#         input_ids = input_ids.to(device)
#         attention_mask = attention_mask.to(device)
#         labels = labels.to(device)

#         optimizer.zero_grad()

#         outputs = model(
#             input_ids=input_ids,
#             attention_mask=attention_mask,
#             labels=labels
#         )

#         loss = outputs.loss
#         total_loss += loss.item()

#         loss.backward()
#         optimizer.step()

#     average_loss = total_loss / len(train_loader)

#     print(f"Epoch {epoch + 1}/{epochs} - Loss: {average_loss:.4f}")

# # Evaluation
# model.eval()
# predictions = []
# true_labels = []

# with torch.no_grad():
#     for batch in test_loader:
#         input_ids, attention_mask, labels = batch
#         input_ids = input_ids.to(device)
#         attention_mask = attention_mask.to(device)

#         outputs = model(
#             input_ids=input_ids,
#             attention_mask=attention_mask
#         )

#         logits = outputs.logits
#         predicted_labels = torch.argmax(logits, dim=1).cpu().numpy()

#         predictions.extend(predicted_labels)
#         true_labels.extend(labels.cpu().numpy())

# accuracy = accuracy_score(true_labels, predictions)
# precision = precision_score(true_labels, predictions)
# recall = recall_score(true_labels, predictions)
# confusion = confusion_matrix(true_labels, predictions)

# print("---Scores---")
# print(f"Accuracy Score: {accuracy * 100:.2f}%")
# print(f"Precision Score: {precision * 100:.2f}%")
# print(f"Recall Score: {recall * 100:.2f}%")

# # Confusion Matrix
# plt.figure(figsize=(10, 6))
# sns.heatmap(confusion, annot=True, cmap='YlGnBu', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])
# plt.xlabel("Predicted Values")
# plt.ylabel("Actual Values")
# plt.show()
